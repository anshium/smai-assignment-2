Q1: Multi-Layer Perceptron (60 Marks)
	1.1. [ ] Multi-Class Classifier
		1.1.1. [ ] Dataset
		1.1.2. [ ] Model Development from Scratch
		1.1.3. [ ] Hyperparameter Tuning and Evaluation with 10-Fold Validation
			- [ ] Hyperparameters
			- [ ] Result Aggregation and Visualisation
			- [ ] Report Questions
	1.2. [ ] MLP Regressor for Price Prediction in Bangalore.
		1.2.1. [ ] Dataset
		1.2.2. [ ] Data Preprocessing
		1.2.3. [ ] Model Development from Scratch
		1.2.4. [ ] Hyperparamter Tuning and Evaluation

	1.3. [ ] Multi-Label News Article Classification
		1.3.1. [ ] Data Preprocessing
		1.3.2. [ ] Model Development
		1.3.3. [ ] Hyperparamter Tuning and Evaluation

Q2: Gaussian Mixture Model (20 Marks)
	2.1. [ ] Implement GMM from Scratch
	2.2. [ ] Segment and Visualise
	2.3. [ ] Visualise the Frequency of points vs Intensity Graph for all three labels. + other things

Q3: PCA (30 marks)
	3.1. [ ] Explained Variance and Lossy Reconstruction.
	3.2. [ ] Classification Performance with vs without dimensionality reduction.
	3.3. [ ] Report

Q4: Autoencoder (30 Marks)
	4.1. [x] Implement the encoder and decoder networks using PyTorch.
	4.2. [x] Last Digit of roll no wali cheeze.
	4.3. [x] Compute and plot histogram of reconstruction error for both normal and anomalous digits.
	4.4. [x] Choose a threshold for anomaly detection based on reconstruction error dist.
	4.5. [x] Evaluate model performance using: Precision, Recall, F1-score.
	4.6. [x] Hyperparameter Tuning
	[ ] Finalise Everything!

Q5: Variational Autoencoder (30 Marks)
	5.1. [ ] Implement and Train a VAE on the MNIST dataset.
	5.2. [ ] What happens when we remove the reconstruction loss? Train VAE without the recon loss and visualize the latent space.
	5.3. [ ] Repeat the above step with KL Divergence Loss.
	5.4. [ ] Keep the latent space dim as 2, sample points from a 2D ...
	5.5. [ ] Replace the BCE with an MSE loss and viz a grid similar to the above question.

