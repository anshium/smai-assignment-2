{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Multi-Class Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions and Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = torch.exp(x - torch.max(x, dim=1, keepdim=True).values)\n",
    "    return exp_x / torch.sum(exp_x, dim=1, keepdim=True)\n",
    "\n",
    "class MLPClassifier:\n",
    "    def __init__(self, input_size, hidden_layers, output_size, activation='relu', learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation\n",
    "        self.output_size = output_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.layers = [input_size] + hidden_layers + [output_size]\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.weights.append(torch.randn(self.layers[i], self.layers[i + 1], device=self.device) * 0.01)\n",
    "            self.biases.append(torch.zeros((1, self.layers[i + 1]), device=self.device))\n",
    "\n",
    "    def _activate(self, x):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "        elif self.activation == 'tanh':\n",
    "            return torch.tanh(x)\n",
    "        elif self.activation == 'relu':\n",
    "            return F.relu(x)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function\")\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "\n",
    "        for i in range(len(self.weights) - 1):  # Hidden layers\n",
    "            z = torch.matmul(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            zs.append(z)\n",
    "            activations.append(self._activate(z))\n",
    "\n",
    "        z = torch.matmul(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        zs.append(z)\n",
    "        activations.append(softmax(z))  # softmax for multi-class classification\n",
    "        \n",
    "        return activations, zs\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        return -torch.mean(torch.sum(y_true * torch.log(y_pred + 1e-9), dim=1))\n",
    "\n",
    "    def backward_propagation(self, X, y, activations, zs):\n",
    "        m = X.shape[0]\n",
    "        deltas = [activations[-1] - y]\n",
    "\n",
    "        for i in range(len(self.weights) - 1, 0, -1):\n",
    "            dz = torch.matmul(deltas[0], self.weights[i].T)\n",
    "            da = torch.where(zs[i - 1] > 0, 1, 0) if self.activation == 'relu' else activations[i] * (1 - activations[i])\n",
    "            delta = dz * da\n",
    "            deltas.insert(0, delta)\n",
    "\n",
    "        dW = [torch.matmul(activations[i].T, deltas[i]) / m for i in range(len(self.weights))]\n",
    "        dB = [torch.sum(deltas[i], dim=0, keepdim=True) / m for i in range(len(self.weights))]\n",
    "\n",
    "        return dW, dB\n",
    "\n",
    "    def update_weights(self, dW, dB):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * dW[i]\n",
    "            self.biases[i] -= self.learning_rate * dB[i]\n",
    "\n",
    "    def train(self, X, y, X_test, y_test, epochs=1000, method='batch', batch_size=32):\n",
    "        if method not in ['batch', 'sgd', 'mini-batch']:\n",
    "            raise ValueError(\"Invalid method! Choose 'batch', 'sgd', or 'mini-batch'.\")\n",
    "        \n",
    "        X = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        y = torch.tensor(y, dtype=torch.float32, device=self.device)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32, device=self.device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
    "            if method == 'batch':\n",
    "                activations, zs = self.forward_propagation(X)\n",
    "                loss = self.compute_loss(y, activations[-1])\n",
    "                dW, dB = self.backward_propagation(X, y, activations, zs)\n",
    "                self.update_weights(dW, dB)\n",
    "\n",
    "            elif method == 'mini-batch':\n",
    "                indices = torch.randperm(X.shape[0])\n",
    "                for i in range(0, X.shape[0], batch_size):\n",
    "                    batch_indices = indices[i:i + batch_size]\n",
    "                    X_batch, y_batch = X[batch_indices], y[batch_indices]\n",
    "                    activations, zs = self.forward_propagation(X_batch)\n",
    "                    dW, dB = self.backward_propagation(X_batch, y_batch, activations, zs)\n",
    "                    self.update_weights(dW, dB)\n",
    "\n",
    "                loss = self.compute_loss(y, self.forward_propagation(X)[0][-1])\n",
    "\n",
    "            elif method == 'sgd':  # Stochastic GD\n",
    "                indices = torch.randperm(X.shape[0])\n",
    "                for i in tqdm(range(X.shape[0]), desc=\"Samples\"):\n",
    "                    X_sample = X[indices[i]].unsqueeze(0)\n",
    "                    y_sample = y[indices[i]].unsqueeze(0)\n",
    "                    activations, zs = self.forward_propagation(X_sample)\n",
    "                    dW, dB = self.backward_propagation(X_sample, y_sample, activations, zs)\n",
    "                    self.update_weights(dW, dB)\n",
    "\n",
    "                loss = self.compute_loss(y, self.forward_propagation(X)[0][-1])\n",
    "\n",
    "            test_activations, _ = self.forward_propagation(X_test)\n",
    "            test_loss = self.compute_loss(y_test, test_activations[-1])\n",
    "\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f\"Epoch {epoch}: Training Loss = {loss.item():.4f}, Testing Loss = {test_loss.item():.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        activations, _ = self.forward_propagation(X)\n",
    "        return torch.argmax(activations[-1], dim=1).cpu().numpy()\n",
    "\n",
    "    def save_model(self, path=\"mlp_model.pth\"):\n",
    "        model_state = {\n",
    "            \"weights\": self.weights,\n",
    "            \"biases\": self.biases\n",
    "        }\n",
    "        torch.save(model_state, path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    def load_model(self, path=\"mlp_model.pth\"):\n",
    "        model_state = torch.load(path, map_location=self.device)\n",
    "        self.weights = model_state[\"weights\"]\n",
    "        self.biases = model_state[\"biases\"]\n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "    def compute_accuracy(self, X, y):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        y = torch.tensor(y, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        predictions = self.predict(X)\n",
    "        y_true = torch.argmax(y, dim=1).cpu().numpy()\n",
    "\n",
    "        accuracy = np.mean(predictions == y_true) * 100\n",
    "        return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_path, image_folder):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    images = []\n",
    "    labels = data['symbol_id'].values\n",
    "\n",
    "    for path in tqdm(data['path'], desc=f\"Loading images from {csv_path}\"):\n",
    "        img = cv2.imread(os.path.join(image_folder, os.path.basename(path)), cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (32, 32)).flatten()\n",
    "        images.append(img / 255.0)\n",
    "\n",
    "    return np.array(images), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images from classification-task/fold-1/train.csv: 100%|██████████| 151241/151241 [00:06<00:00, 23529.98it/s]\n",
      "Loading images from classification-task/fold-1/test.csv: 100%|██████████| 16992/16992 [00:00<00:00, 26545.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoding\n",
    "def preprocess_labels(labels):\n",
    "    encoder = LabelEncoder()\n",
    "    labels_encoded = encoder.fit_transform(labels)\n",
    "    one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "    labels_one_hot = one_hot_encoder.fit_transform(labels_encoded.reshape(-1, 1))\n",
    "    return labels_one_hot, encoder\n",
    "\n",
    "train_csv = 'classification-task/fold-1/train.csv'\n",
    "test_csv = 'classification-task/fold-1/test.csv'\n",
    "image_folder = 'images'\n",
    "\n",
    "X_train, y_train = load_data(train_csv, image_folder)\n",
    "X_test, y_test = load_data(test_csv, image_folder)\n",
    "y_train, encoder = preprocess_labels(y_train)\n",
    "y_test, _ = preprocess_labels(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 15/10000 [00:00<02:51, 58.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss = 5.9108, Testing Loss = 5.9104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 1015/10000 [01:14<06:46, 22.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000: Training Loss = 5.3607, Testing Loss = 5.3724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 2015/10000 [02:10<06:03, 21.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2000: Training Loss = 4.4282, Testing Loss = 4.3631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 3015/10000 [03:06<05:19, 21.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3000: Training Loss = 3.2728, Testing Loss = 3.2170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 4015/10000 [04:02<04:32, 21.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4000: Training Loss = 2.4346, Testing Loss = 2.3726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 5015/10000 [04:58<03:45, 22.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5000: Training Loss = 1.9522, Testing Loss = 1.9789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 6015/10000 [05:55<03:00, 22.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6000: Training Loss = 1.7183, Testing Loss = 1.6836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 7015/10000 [06:51<02:16, 21.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7000: Training Loss = 1.5054, Testing Loss = 1.4806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 8015/10000 [07:48<01:30, 22.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8000: Training Loss = 1.4199, Testing Loss = 1.4146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 9015/10000 [08:44<00:44, 21.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9000: Training Loss = 1.3436, Testing Loss = 1.3434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10000/10000 [09:40<00:00, 17.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trained_mlp_batch.pth\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(input_size=1024, hidden_layers=[64, 32], output_size=y_train.shape[1], activation='relu', learning_rate=0.1)\n",
    "mlp.train(X_train, y_train, X_test, y_test, epochs=10000, method='batch')\n",
    "\n",
    "mlp.save_model(\"trained_mlp_batch.pth\")\n",
    "\n",
    "predictions = mlp.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
