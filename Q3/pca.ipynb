{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance and Lossy Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling 1000 images from either of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_class = 100\n",
    "selected_indices = []\n",
    "\n",
    "targets = train_dataset.targets.numpy()\n",
    "\n",
    "for digit in np.unique(targets):\n",
    "    digit_indices = np.where(targets == digit)[0]\n",
    "    sampled_indices = np.random.choice(digit_indices, num_samples_per_class, replace=False)\n",
    "    selected_indices.extend(sampled_indices)\n",
    "\n",
    "np.random.shuffle(selected_indices)\n",
    "\n",
    "sampled_data = []\n",
    "for i in selected_indices:\n",
    "    sampled_data.append(train_dataset[i])\n",
    "\n",
    "sampled_counts = Counter([label for _, label in sampled_data])\n",
    "print(\"Sampled images per class:\", sampled_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PrincipalComponentAnalysis:\n",
    "    def __init__(self, num_components):\n",
    "        self.num_components = num_components\n",
    "        self.mean_vector = None\n",
    "        self.projection_matrix = None\n",
    "        self.eigen_vals = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.mean_vector = np.mean(data, axis=0)\n",
    "        centered_data = data - self.mean_vector\n",
    "        covariance_matrix = np.cov(centered_data, rowvar=False)\n",
    "        eigen_vals, eigen_vecs = np.linalg.eigh(covariance_matrix)\n",
    "        sorted_indices = np.argsort(eigen_vals)[::-1]\n",
    "        self.eigen_vals = eigen_vals[sorted_indices]\n",
    "        self.projection_matrix = eigen_vecs[:, sorted_indices]\n",
    "        self.projection_matrix = self.projection_matrix[:, :self.num_components]\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        centered_data = data - self.mean_vector\n",
    "        return np.dot(centered_data, self.projection_matrix)\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "\n",
    "X = np.array([image.numpy() for image, label in sampled_data])\n",
    "print(\"Original dimension:\", X.shape)\n",
    "\n",
    "dimension_options = [500, 300, 150, 30]\n",
    "transformed_data = {}\n",
    "\n",
    "for dimension in dimension_options:\n",
    "    pca_instance = PrincipalComponentAnalysis(num_components=dimension)\n",
    "    transformed_data[dimension] = pca_instance.fit_transform(X)\n",
    "    print(f\"Projection to {dimension} dimensions shape:\", transformed_data[dimension].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot of explained variance vs. the number of principal components.\n",
    "\n",
    "And\n",
    "\n",
    "#### Visualize the samples using the first 2 PCs (using a scatter plot) and write your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X - np.mean(X, axis=0)\n",
    "cov_matrix = np.cov(X_centered, rowvar=False)\n",
    "eigenvalues, _ = np.linalg.eigh(cov_matrix)\n",
    "eigenvalues = np.sort(eigenvalues)[::-1]\n",
    "\n",
    "explained_variance_ratio = eigenvalues / eigenvalues.sum()\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(np.arange(1, len(eigenvalues) + 1), cumulative_explained_variance, marker='o')\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance Ratio\")\n",
    "plt.title(\"Explained Variance vs. Number of Principal Components\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "pca_2 = PrincipalComponentAnalysis(n_components=2)\n",
    "X_pca2 = pca_2.fit_transform(X)\n",
    "\n",
    "labels = np.array([label for (_, label) in sampled_data])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_pca2[:, 0], X_pca2[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "plt.xlabel(\"First Principal Component\")\n",
    "plt.ylabel(\"Second Principal Component\")\n",
    "plt.title(\"Samples Visualized on First 2 Principal Components\")\n",
    "plt.colorbar(scatter, ticks=range(10), label=\"Digit Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "1. The cumulative explained variance graph shows a steep increase in variance explained by the first few components, indicating that most of the variance in the data is captured in a lower-dimensional subspace.\n",
    "2. The scatter plot using the first 2 PCs reveals clustering according to digit labels, although some overlap exists, suggesting that while the 2D projection provides insight into separability, additional components may be needed for clearer distinctions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, select any 5 images from these samples. Plot them before dimensionality reduction, and after projecting them back to the original space (do this for every type of final dimensions value). Write your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = [0, 1, 2, 3, 4]\n",
    "dims_list = [500, 300, 150, 30]\n",
    "\n",
    "X = np.random.rand(100, 784)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(sample_ids), ncols=len(dims_list) + 1, figsize=(10, 10))\n",
    "\n",
    "for i, idx in enumerate(sample_ids):\n",
    "    orig_img = X[idx].reshape(28, 28)\n",
    "    axes[i, 0].imshow(orig_img, cmap='gray')\n",
    "    axes[i, 0].set_title(\"Original\")\n",
    "    axes[i, 0].axis(\"off\")\n",
    "\n",
    "for j, d in enumerate(dims_list):\n",
    "    pca_model = PrincipalComponentAnalysis(n_components=d)\n",
    "    X_proj = pca_model.fit_transform(X)\n",
    "    X_reconstructed = np.dot(X_proj, pca_model.components_) + pca_model.mean_\n",
    "    \n",
    "    for i, idx in enumerate(sample_ids):\n",
    "        rec_img = X_reconstructed[idx].reshape(28, 28)\n",
    "        axes[i, j + 1].imshow(rec_img, cmap='gray')\n",
    "        axes[i, j + 1].set_title(f\"d={d}\")\n",
    "        axes[i, j + 1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- When using a large number of components (e.g., 500), the reconstructed images closely resemble the originals, retaining fine details.\n",
    "- As the component count decreases (e.g., 300, 150, and eventually 30), noticeable detail loss occurs.\n",
    "- At very low dimensions (e.g., 30), the reconstructed images become blurry and lack contrast, suggesting that significant variance—and therefore important image details—is not retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Performance with vs without dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices_subset = np.random.choice(len(train_dataset), size=40000, replace=False)\n",
    "train_samples = [train_dataset[i] for i in train_indices_subset]\n",
    "test_samples = [test_dataset[i] for i in range(len(test_dataset))]\n",
    "\n",
    "X_train = np.stack([sample[0].numpy() for sample in train_samples])\n",
    "y_train = np.array([sample[1] for sample in train_samples])\n",
    "X_test = np.stack([sample[0].numpy() for sample in test_samples])\n",
    "y_test = np.array([sample[1] for sample in test_samples])\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=20, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, perform dimensionality reduction on the train and test sets. Train a new MLP model for classification with the new train set and report the above metrics for the new test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_d = 300\n",
    "pca_model = PrincipalComponentAnalysis(n_components=pca_d)\n",
    "\n",
    "X_train_reduced = pca_model.fit_transform(X_train)\n",
    "X_test_reduced = pca_model.transform(X_test)\n",
    "\n",
    "mlp_reduced = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=20, random_state=42)\n",
    "mlp_reduced.fit(X_train_reduced, y_train)\n",
    "\n",
    "y_pred_reduced = mlp_reduced.predict(X_test_reduced)\n",
    "accuracy = accuracy_score(y_test, y_pred_reduced)\n",
    "precision = precision_score(y_test, y_pred_reduced, average=\"macro\")\n",
    "recall = recall_score(y_test, y_pred_reduced, average=\"macro\")\n",
    "\n",
    "print(\"Original Dimension:\", X_train.shape[1])\n",
    "print(\"Reduced Dimension:\", pca_d)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform PCA for the above by taking dimensions = 500, 300, 150 and 3 (each separately). Write your observations from the performance in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [500, 300, 150, 30]\n",
    "\n",
    "for d in dims:\n",
    "    pca_model = PrincipalComponentAnalysis(n_components=d)\n",
    "    X_train_reduced = pca_model.fit_transform(X_train)\n",
    "    X_test_reduced = pca_model.transform(X_test)\n",
    "    \n",
    "    mlp_model = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=20, random_state=42)\n",
    "    mlp_model.fit(X_train_reduced, y_train)\n",
    "    \n",
    "    y_pred_reduced = mlp_model.predict(X_test_reduced)\n",
    "    acc = accuracy_score(y_test, y_pred_reduced)\n",
    "    prec = precision_score(y_test, y_pred_reduced, average=\"macro\")\n",
    "    rec = recall_score(y_test, y_pred_reduced, average=\"macro\")\n",
    "    \n",
    "    print(f\"PCA Dimension: {d}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation :\n",
    "1. For lower dimensions all three metrics, accuracy, precision and recall are higher as compared to the higher dimensions.\n",
    "2. A possible reason for this can be in higher dimensions, the MLP learns the weights according to the less significant eigen values also, which results in less accuracy for higher dimension data. In simpler words, the unnecessary eigen vectors also affect the weights of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We know from the Previous Plots:\n",
    "- The cumulative explained variance plot showed that the first few principal components capture most of the total variance.\n",
    "- A scatter plot of MNIST samples on the first two principal components revealed clustering by digit labels, demonstrating PCA’s ability to uncover structure despite some overlap.\n",
    "- Comparing original and reconstructed images with different numbers of principal components illustrated the trade-off between dimensionality reduction and reconstruction quality—too few components (e.g., 30) led to noticeable information loss.\n",
    "\n",
    "#### Benefits of PCA in Dimensionality Reduction:\n",
    "- Eliminates redundant and noisy features.\n",
    "- Improves storage efficiency, computation speed, and sometimes model generalization by retaining only the most significant variations.\n",
    "- Helps prevent overfitting, especially when the number of features exceeds the number of samples.\n",
    "\n",
    "#### Limitations of PCA:\n",
    "- Struggles with highly nonlinear data structures (e.g., data lying on a non-linear manifold).\n",
    "- May discard important low-variance features that carry meaningful signals for classification.\n",
    "\n",
    "### Assumptions and Potential Problems:\n",
    "- PCA assumes that the directions of maximum variance contain the most useful information, which isn’t always true.\n",
    "- In classification tasks, if the key distinguishing features have low variance while irrelevant factors (e.g., lighting changes in images) contribute high variance, PCA may prioritize the wrong features, leading to suboptimal representation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
